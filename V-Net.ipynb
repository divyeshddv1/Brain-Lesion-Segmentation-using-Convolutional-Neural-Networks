{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import os\n",
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Function, Variable\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla K80'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELU(elu, nchan):\n",
    "    if elu:\n",
    "        return nn.ELU(inplace=True)\n",
    "    else:\n",
    "        return nn.PReLU(nchan)\n",
    "    \n",
    "def n_conv(nchan, depth, elu):\n",
    "    layers = []\n",
    "    for _ in range(depth):\n",
    "        layers.append(single_conv(nchan, elu))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class single_conv(nn.Module):\n",
    "    def __init__(self, nchan, elu):\n",
    "        super(single_conv, self).__init__()\n",
    "        self.relu = ELU(elu, nchan)\n",
    "        self.conv = nn.Conv3d(nchan, nchan, kernel_size=5, padding=2)\n",
    "        self.bn = nn.BatchNorm3d(nchan)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn(self.conv(x)))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class input_layer(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, elu):\n",
    "        super(input_layer, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv3d(in_ch,out_ch,kernel_size=5,padding=2)\n",
    "        self.relu = ELU(elu, out_ch)\n",
    "        self.bn = nn.BatchNorm3d(out_ch)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 1. Convolve\n",
    "        out = self.conv(x)\n",
    "        # 2. Normalize\n",
    "        out = self.bn(out)\n",
    "        # 3. Concat output accross 16 channels\n",
    "        #x16 = torch.cat((x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x), 0)\n",
    "        # 4. Add concatenated output and input\n",
    "        out = torch.add(out, x)\n",
    "        # 5. Activation\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class down_layer(nn.Module):\n",
    "    def __init__(self, in_ch, nConv, elu):\n",
    "        super(down_layer, self).__init__()\n",
    "        out_ch = 2*in_ch\n",
    "        self.down_conv = nn.Conv3d(in_ch, out_ch, kernel_size=2, stride=2)\n",
    "        self.bn = nn.BatchNorm3d(out_ch)\n",
    "        self.relu = ELU(elu, out_ch)\n",
    "        self.layers = n_conv(out_ch, nConv, elu)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        down = self.relu(self.bn(self.down_conv(x)))\n",
    "        out = self.layers(down)\n",
    "        out = self.relu(torch.add(out, down))\n",
    "        return down, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class up_layer(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, nConv, elu):\n",
    "        super(up_layer, self).__init__()\n",
    "        self.up_conv = nn.ConvTranspose3d(in_ch, out_ch//2 , kernel_size=2, stride=2)\n",
    "        self.bn = nn.BatchNorm3d(out_ch//2 )\n",
    "        self.relu1 = ELU(elu, out_ch//2 )\n",
    "        self.relu2 = ELU(elu, out_ch)\n",
    "        self.layers = n_conv(out_ch, nConv, elu)\n",
    "        #self.do2 = nn.Dropout3d()\n",
    "\n",
    "    def forward(self, x, skipx):\n",
    "        out = self.relu1(self.bn(self.up_conv(x)))\n",
    "        #print('after out', out.shape)\n",
    "        #skipxdo = self.do2(skipx)\n",
    "        #print('after do2', skipxdo.shape)\n",
    "        xcat = torch.cat((out, skipx), 1)\n",
    "        #print('after xcat', xcat.shape)\n",
    "        out = self.layers(xcat)\n",
    "        #print('after layers', out.shape)\n",
    "        out = self.relu2(torch.add(out, xcat))\n",
    "        #print('after relu2', out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class output_layer(nn.Module):\n",
    "    def __init__(self, in_ch, elu, nll):\n",
    "        super(output_layer, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_ch, 2, kernel_size=5, padding=2)\n",
    "        self.bn = nn.BatchNorm3d(2)\n",
    "        self.conv2 = nn.Conv3d(2, 2, kernel_size=1)\n",
    "        self.relu1 = ELU(elu, 2)\n",
    "        if nll:\n",
    "            self.softmax = F.log_softmax\n",
    "        else:\n",
    "            self.softmax = F.softmax\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu1(self.bn(self.conv1(x)))\n",
    "        out = self.conv2(out)\n",
    "        #out = out.permute(0, 2, 3, 4, 1).contiguous()\n",
    "        #out = out.view(out.numel() // 2, 2)\n",
    "        out = self.softmax(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, elu=True, nll=False):\n",
    "        super(VNet, self).__init__()\n",
    "        #In\n",
    "        self.input = input_layer(1, 16, elu)\n",
    "        \n",
    "        #Down\n",
    "        self.down32 = down_layer(16, 2, elu)\n",
    "        self.down64 = down_layer(32, 3, elu)\n",
    "        self.down128 = down_layer(64, 3, elu)\n",
    "        self.down256 = down_layer(128, 3, elu)\n",
    "        \n",
    "        #Up\n",
    "        self.up256 = up_layer(256,256, 3, elu)\n",
    "        self.up128 = up_layer(256,128, 3, elu)\n",
    "        self.up64 = up_layer(128,64, 2, elu)\n",
    "        self.up32 = up_layer(64,32, 1, elu)\n",
    "        \n",
    "        #Out\n",
    "        self.output = output_layer(32, elu, nll)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #Layer 1: In\n",
    "        out16 = self.input(x)\n",
    "        \n",
    "        #Layer 2 : Down ( 2 conv layers deep)\n",
    "        d_32, out32 = self.down32(out16)\n",
    "        \n",
    "        #Layer 3 : Down ( 3 conv layers deep)\n",
    "        d_64, out64 = self.down64(out32)\n",
    "        \n",
    "        #Layer 4 : Down ( 3 conv layers deep)\n",
    "        d_128, out128 = self.down128(out64)      \n",
    "        \n",
    "        #Layer 5 : Down ( 3 conv layers deep)\n",
    "        d_256, out256 = self.down256(out128)\n",
    "        \n",
    "        #Layer 5 : up ( 3 conv layers deep)\n",
    "        output = self.up256(out256, out128)\n",
    "        #print(output.shape)\n",
    "        #Layer 4 : up ( 3 conv layers deep)\n",
    "        output = self.up128(output, out64)\n",
    "        \n",
    "        #Layer 3 : up ( 3 conv layers deep)\n",
    "        output = self.up64(output, out32)\n",
    "        \n",
    "        #Layer 2 : up ( 2 conv layers deep)\n",
    "        output = self.up32(output, out16)\n",
    "        \n",
    "        #Layer 1 : out\n",
    "        output = self.output(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = VNet(elu=True, nll=False)\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network:  65186716\n"
     ]
    }
   ],
   "source": [
    "n_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print('Number of parameters in network: ', n_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder that contains folders of segmentation data\n",
    "PATH = \"data/TrainingDataset_MSSEG/\"\n",
    "# Takes all folders in the path \n",
    "PATH = PATH + \"*/\"\n",
    "image_paths = []\n",
    "mask_paths = []\n",
    "\n",
    "block_size = (32,32,32)\n",
    "\n",
    "directory_paths = glob(PATH)\n",
    "for path in directory_paths:\n",
    "    # Load all the paths for each Flair set of data (1 Flair data and all its segmentation paths)\n",
    "    flair_path = path + '3DFLAIR.nii.gz'\n",
    "    seg1_path = path + 'ManualSegmentation_1.nii.gz'\n",
    "    seg2_path = path + 'ManualSegmentation_2.nii.gz'\n",
    "    seg3_path = path + 'ManualSegmentation_3.nii.gz'\n",
    "    seg4_path = path + 'ManualSegmentation_4.nii.gz'\n",
    "    seg5_path = path + 'ManualSegmentation_5.nii.gz'\n",
    "    seg6_path = path + 'ManualSegmentation_6.nii.gz'\n",
    "    seg7_path = path + 'ManualSegmentation_7.nii.gz'\n",
    "    image_paths.extend([flair_path,flair_path,flair_path,flair_path,flair_path,flair_path,flair_path])\n",
    "    mask_paths.extend([seg1_path,seg2_path,seg3_path,seg4_path,seg5_path,seg6_path,seg7_path])\n",
    "    \n",
    "#print(image_paths)\n",
    "#print(mask_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val(image_paths, mask_paths, train_size):\n",
    "    img_paths_dic = {}\n",
    "    mask_paths_dic = {}\n",
    "    len_data = len(image_paths)\n",
    "    print('total len:', len_data)\n",
    "    p = 0\n",
    "    q = 0\n",
    "    for i in range(len(image_paths)):\n",
    "        img_paths_dic[str(p)+'_'+str(q)] = image_paths[i]\n",
    "        if q==6:\n",
    "            q=0\n",
    "            p=p+1\n",
    "        else:\n",
    "            q=q+1\n",
    "    \n",
    "    p = 0\n",
    "    q = 0\n",
    "    for i in range(len(mask_paths)):\n",
    "        mask_paths_dic[str(p)+'_'+str(q)] = mask_paths[i]\n",
    "        if q==6:\n",
    "            q=0\n",
    "            p=p+1\n",
    "        else:\n",
    "            q=q+1\n",
    "        \n",
    "    img_mask_list = []\n",
    "    #print(img_paths_dic)\n",
    "    \n",
    "    for key in img_paths_dic:\n",
    "        img_mask_list.append((img_paths_dic[key], mask_paths_dic[key]))\n",
    "        \n",
    "    train_img_mask_paths = img_mask_list[:int(len_data*train_size)] \n",
    "    val_img_mask_paths = img_mask_list[int(len_data*train_size):]\n",
    "    return train_img_mask_paths, val_img_mask_paths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_padding(data, block_size):\n",
    "    # Calculate final size to be achieved\n",
    "    ceil_val = math.ceil(data.shape[0]/block_size[0])\n",
    "    #Calculate required padding size \n",
    "    pad_val_c = (block_size[0] * ceil_val) - data.shape[0]\n",
    "    \n",
    "    # Calculate final size to be achieved\n",
    "    ceil_val = math.ceil(data.shape[1]/block_size[1])\n",
    "    #Calculate required padding size\n",
    "    pad_val_h = (block_size[1] * ceil_val) - data.shape[1]\n",
    "    \n",
    "    # Calculate final size to be achieved\n",
    "    ceil_val = math.ceil(data.shape[2]/block_size[2])\n",
    "    # Calculate required padding size\n",
    "    pad_val_w = (block_size[2] * ceil_val) - data.shape[2]\n",
    "    \n",
    "    # Constant padding\n",
    "    #data = data.numpy()\n",
    "    data = np.pad(data, ((0,pad_val_c),(0,pad_val_h),(0,pad_val_w)), 'constant')\n",
    "    #data = np.array(data, dtype=np.int16)\n",
    "    \n",
    "    #changed dtype to float\n",
    "    data = np.array(data, dtype=np.float32)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_data_blocks(data, block_size ):\n",
    "    x = torch.from_numpy(data)\n",
    "    # Add a dimension at 0th position\n",
    "    x = x.unsqueeze(0)\n",
    "    # Kernel Size\n",
    "    kc, kh, kw = block_size[0], block_size[1], block_size[2]\n",
    "    # stride\n",
    "    dc, dh, dw = block_size[0], block_size[1], block_size[2]\n",
    "    patches = x.unfold(1, kc, dc).unfold(2, kh, dh).unfold(3, kw, dw)\n",
    "    unfold_shape = patches.size()\n",
    "    patches = patches.contiguous().view(patches.size(0), -1, kc, kh, kw)\n",
    "    #Return Patches and Unfold Shape\n",
    "    return patches, unfold_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from collections import defaultdict as dd\\n\\ndef preprocess_image(image_mask_paths):\\n    img_mask_list = []\\n\\n    for i in tqdm(range(len(image_mask_paths))):\\n        vol = nib.load(image_mask_paths[i][0])\\n        m = nib.load(image_mask_paths[i][1])\\n        img = np.array(vol.get_data(), np.float32) / 255.0\\n        mask = np.array(m.get_data(),np.uint8)\\n        img_mask_list.append((img, mask))\\n\\n    return img_mask_list'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from collections import defaultdict as dd\n",
    "\n",
    "def preprocess_image(image_mask_paths):\n",
    "    img_mask_list = []\n",
    "\n",
    "    for i in tqdm(range(len(image_mask_paths))):\n",
    "        vol = nib.load(image_mask_paths[i][0])\n",
    "        m = nib.load(image_mask_paths[i][1])\n",
    "        img = np.array(vol.get_data(), np.float32) / 255.0\n",
    "        mask = np.array(m.get_data(),np.uint8)\n",
    "        img_mask_list.append((img, mask))\n",
    "\n",
    "    return img_mask_list\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_image(image_mask_paths):\n",
    "    img_mask_list = []\n",
    "\n",
    "    for i in tqdm(range(len(image_mask_paths))):\n",
    "        vol = nib.load(image_mask_paths[i][0])\n",
    "        m = nib.load(image_mask_paths[i][1])\n",
    "        img = np.array(vol.get_data(), np.float32) / 255.0\n",
    "        img_padded = zero_padding(img, block_size)\n",
    "        mask = np.array(m.get_data(),np.uint8)\n",
    "        mask_padded = zero_padding(mask, block_size)\n",
    "\n",
    "        # Generate data blocks of block_size\n",
    "        img_blocks, unfold_shape_img = get_data_blocks(data = img_padded, block_size = block_size)\n",
    "        mask_blocks, unfold_shape_mask = get_data_blocks(data = mask_padded, block_size = block_size)\n",
    "\n",
    "        img_array = img_blocks.numpy()\n",
    "        mask_array = mask_blocks.numpy()\n",
    "        per_img = []\n",
    "        per_mask = []\n",
    "\n",
    "        for i in range(len(img_array[0])):\n",
    "            img_mask_list.append((img_array[0][i], mask_array[0][i]))\n",
    "            #print(img_array[0][i].shape)\n",
    "\n",
    "    return img_mask_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/99 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total len: 105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [02:13<00:00,  1.35s/it]\n",
      "100%|██████████| 6/6 [00:11<00:00,  1.88s/it]\n"
     ]
    }
   ],
   "source": [
    "train_img_mask_paths, val_img_mask_paths = split_train_val(image_paths, mask_paths, 0.95)\n",
    "\n",
    "\n",
    "#Training:\n",
    "train_img_masks = preprocess_image(train_img_mask_paths)\n",
    "\n",
    "#Validation:\n",
    "val_img_masks = preprocess_image(val_img_mask_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"train_img_mask_paths, val_img_mask_paths = split_train_val(image_paths, mask_paths, 0.95)\\n\\ndef pickle_store(file_name,save_data):\\n    fileObj = open(file_name,'wb')\\n    pickle.dump(save_data,fileObj)\\n    fileObj.close()\\n\\ntrain_img_masks_save_path = './train_img_masks.pickle'\\nif os.path.exists(train_img_masks_save_path):\\n    with open(train_img_masks_save_path,'rb') as f:\\n        train_img_masks = pickle.load(f)\\n    f.close()\\nelse:\\n    train_img_masks = preprocess_image(train_img_mask_paths)\\n    pickle_store(train_img_masks_save_path, train_img_masks)\\nprint('train len: {}'.format(len(train_img_masks)))\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"train_img_mask_paths, val_img_mask_paths = split_train_val(image_paths, mask_paths, 0.95)\n",
    "\n",
    "def pickle_store(file_name,save_data):\n",
    "    fileObj = open(file_name,'wb')\n",
    "    pickle.dump(save_data,fileObj)\n",
    "    fileObj.close()\n",
    "\n",
    "train_img_masks_save_path = './train_img_masks.pickle'\n",
    "if os.path.exists(train_img_masks_save_path):\n",
    "    with open(train_img_masks_save_path,'rb') as f:\n",
    "        train_img_masks = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    train_img_masks = preprocess_image(train_img_mask_paths)\n",
    "    pickle_store(train_img_masks_save_path, train_img_masks)\n",
    "print('train len: {}'.format(len(train_img_masks)))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val len: 6\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# For validation data\n",
    "val_img_masks_save_path = './val_img_masks.pickle'\n",
    "if os.path.exists(val_img_masks_save_path):\n",
    "    with open(val_img_masks_save_path,'rb') as f:\n",
    "        val_img_masks = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    val_img_masks = preprocess_image(val_img_mask_paths)\n",
    "    pickle_store(val_img_masks_save_path,val_img_masks)\n",
    "print('val len: {}'.format(len(val_img_masks)))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"\n",
    "    Convert ndarrays in sample to Tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['img'], sample['label']\n",
    "        image = image[None,:,:]\n",
    "        label = label[None,:,:]\n",
    "#         image = image.numpy()\n",
    "#         label = label.numpy()\n",
    "        return {'img': torch.from_numpy(image.copy()).type(torch.FloatTensor),\n",
    "                'label': torch.from_numpy(label.copy()).type(torch.FloatTensor)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_masks, transforms=None):\n",
    "\n",
    "        self.image_masks = image_masks\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):  # return count of sample we have\n",
    "\n",
    "        return len(self.image_masks)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        image = self.image_masks[index][0] # H, W, C\n",
    "        mask = self.image_masks[index][1]\n",
    "\n",
    "#         image = np.transpose(image, axes=[2, 0, 1]) # C, H, W\n",
    "\n",
    "        sample = {'img': image, 'label': mask}\n",
    "\n",
    "        if transforms:\n",
    "            sample = self.transforms(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "train_dataset = CustomDataset(train_img_masks, transforms=transforms.Compose([ToTensor()]))\n",
    "val_dataset = CustomDataset(val_img_masks, transforms=transforms.Compose([ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define dice coefficient \n",
    "class DiceCoeff(Function):\n",
    "    \"\"\"Dice coeff for one pair of input image and target image\"\"\"\n",
    "    def forward(self, prediction, target):\n",
    "        self.save_for_backward(prediction, target)\n",
    "        eps = 0.0001 # in case union = 0\n",
    "        # Calculate intersection and union. \n",
    "        # You can convert the input image into a vector with input.contiguous().view(-1)\n",
    "        # Then use torch.dot(A, B) to calculate the intersection.\n",
    "        A = prediction.view(-1)\n",
    "        B = target.view(-1)\n",
    "        inter = torch.dot(A.float(),B.float())\n",
    "        union = torch.sum(A.float()) + torch.sum(B.float()) - inter + eps\n",
    "        # Calculate DICE \n",
    "        d = inter / union\n",
    "        return d\n",
    "\n",
    "# Calculate dice coefficients for batches\n",
    "def dice_coeff(prediction, target):\n",
    "    \"\"\"Dice coeff for batches\"\"\"\n",
    "    s = torch.FloatTensor(1).zero_()\n",
    "    \n",
    "    # For each pair of input and target, call DiceCoeff().forward(prediction, target) to calculate dice coefficient\n",
    "    # Then average\n",
    "    for i, (a,b) in enumerate(zip(prediction, target)):\n",
    "        s += DiceCoeff().forward(a,b)\n",
    "    s = s / (i + 1)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_net(net, dataset):\n",
    "    # set net mode to evaluation\n",
    "    net.eval()\n",
    "    tot = 0\n",
    "    for i, b in enumerate(dataset):\n",
    "        img = b['img'].to(device)\n",
    "        B = img.shape[0]\n",
    "        true_mask = b['label'].to(device)\n",
    "\n",
    "        # Feed the image to the network to get predicted mask\n",
    "        mask_pred = net(img.float())\n",
    "\n",
    "        # For all pixels in predicted mask, set them to 1 if larger than 0.5. Otherwise set them to 0\n",
    "        mask_pred = mask_pred > 0.5\n",
    "        # calculate dice_coeff()\n",
    "        # note that you should add all the dice_coeff in validation/testing dataset together\n",
    "        # call dice_coeff() here\n",
    "        masks_probs_flat = mask_pred.view(mask_pred.numel())\n",
    "        true_masks_flat = true_mask.view(true_mask.numel())\n",
    "        \n",
    "        tot += dice_coeff(true_masks_flat,masks_probs_flat)\n",
    "        #tot += dice_coeff(true_mask,mask_pred)\n",
    "        # Return average dice_coeff()\n",
    "    return tot / (i + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/5.\n",
      "0.0000 --- loss: 0.462578\n",
      "0.0594 --- loss: 0.088733\n",
      "0.1187 --- loss: 0.041544\n",
      "0.1781 --- loss: 0.050537\n",
      "0.2375 --- loss: 0.046092\n",
      "0.2968 --- loss: 0.019617\n",
      "0.3562 --- loss: 0.017183\n",
      "0.4156 --- loss: 0.014051\n",
      "0.4749 --- loss: 0.015132\n",
      "0.5343 --- loss: 0.012398\n",
      "0.5937 --- loss: 0.009023\n",
      "0.6530 --- loss: 0.021645\n",
      "0.7124 --- loss: 0.018714\n",
      "0.7718 --- loss: 0.009562\n",
      "0.8311 --- loss: 0.006119\n",
      "0.8905 --- loss: 0.008618\n",
      "0.9499 --- loss: 0.009732\n",
      "Epoch finished ! Loss: 0.055050799899652704\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "epochs = 5 # e.g. 10, or more until dice converge\n",
    "batch_size = 100 # e.g. 16\n",
    "lr = 0.01        # e.g. 0.01\n",
    "N_train = len(train_img_masks)\n",
    "model_save_path = 'model/'  # directory to same the model after each epoch.\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(),lr = lr,momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "net.to(device)\n",
    "# Start training\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch {}/{}.'.format(epoch + 1, epochs))\n",
    "    net.train()\n",
    "    # Reload images and masks for training and validation and perform random shuffling at the begining of each epoch\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    epoch_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    for i, b in enumerate(train_loader):\n",
    "        # Get images and masks from each batch\n",
    "\n",
    "        imgs = b['img'].to(device)\n",
    "        true_masks = b['label'].to(device)\n",
    "        #print('True mask shape: ',true_masks.shape)\n",
    "        # Feed your images into the network\n",
    "        masks_pred = net.forward(imgs.float())\n",
    "        #print('Predicted mask shape: ',masks_pred.shape)\n",
    "        #masks_pred = nn.functional.interpolate(masks_pred, size=true_masks.shape, mode='bilinear')\n",
    "        # Flatten the predicted masks and true masks. For example, A_flat = A.view(-1)\n",
    "        #masks_probs_flat = masks_pred.view(-1)\n",
    "        #true_masks_flat = true_masks.view(-1)\n",
    "        #print('true_masks: ',true_masks.shape)\n",
    "        #print('masks_pred: ',masks_pred.shape)\n",
    "        masks_probs = masks_pred[:,0,:,:,:]\n",
    "        masks_probs = masks_probs.unsqueeze(1)\n",
    "        #print(masks_probs.shape)\n",
    "        \n",
    "        #masks_probs_flat = np.transpose(masks_probs_flat, axes=[1, 0, 2, 3, 4])\n",
    "        #masks_probs_flat = masks_probs_flat[0]\n",
    "        #masks_probs_flat = masks_probs.view(masks_probs.numel())\n",
    "        #true_masks_flat = true_masks.view(true_masks.numel())\n",
    "        \n",
    "        masks_probs_flat = masks_probs.reshape(1,-1)\n",
    "        masks_probs_flat = masks_probs_flat.squeeze()\n",
    "        \n",
    "        true_masks_flat = true_masks.reshape(1,-1)\n",
    "        true_masks_flat = true_masks_flat.squeeze()\n",
    "        \n",
    "        # Calculate the loss by comparing the predicted masks vector and true masks vector\n",
    "        # And sum the losses together\n",
    "        loss = criterion(masks_probs_flat,true_masks_flat.float())\n",
    "        epoch_loss += loss.item()\n",
    "        if count % 50 == 0:\n",
    "            print('{0:.4f} --- loss: {1:.6f}'.format(i * batch_size / N_train, loss.item()))\n",
    "        count = count + 1\n",
    "        # optimizer.zero_grad() clears x.grad for every parameter x in the optimizer.\n",
    "        # It’s important to call this before loss.backward(), otherwise you’ll accumulate the gradients from multiple passes.\n",
    "        optimizer.zero_grad()\n",
    "        # loss.backward() computes dloss/dx for every parameter x which has requires_grad=True.\n",
    "        # These are accumulated into x.grad for every parameter x\n",
    "        loss.backward()\n",
    "        # optimizer.step updates the value of x using the gradient x.grad.\n",
    "        optimizer.step()\n",
    "    print('Epoch finished ! Loss: {}'.format(epoch_loss / i))\n",
    "\n",
    "    # Perform validation with eval_net() on the validation data\n",
    "    val_dice = eval_net(net,val_loader)\n",
    "    print('Validation Dice Coeff: {}'.format(val_dice))\n",
    "    # Save the model after each epoch\n",
    "    if os.path.isdir(model_save_path):\n",
    "        torch.save(net.state_dict(),model_save_path + 'Brain_Seg_Epoch{}.pth'.format(epoch + 1))\n",
    "    else:\n",
    "        os.makedirs(model_save_path, exist_ok=True)\n",
    "        torch.save(net.state_dict(),model_save_path + 'Brain_Seg_Epoch{}.pth'.format(epoch + 1))\n",
    "    print('Checkpoint {} saved !'.format(epoch + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_img(net,full_img,out_threshold=0.5):\n",
    "    # set the mode of your network to evaluation\n",
    "    net.eval()\n",
    "\n",
    "    # convert from Height*Width*Channel TO Channel*Height*Width\n",
    "    #full_img = np.transpose(full_img,[2,0,1])\n",
    "\n",
    "    # convert numpy array to torch tensor, normalize to range (0,1)\n",
    "    X_img = torch.from_numpy(full_img).unsqueeze(0).unsqueeze(0).to(device)/255\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        output_img = net(X_img.float())\n",
    "        #out_probs = output_img.squeeze(0).squeeze(0)\n",
    "        masks_probs = output_img.mean(0) # removes channel\n",
    "        masks_probs = masks_probs[0] # consider output[0]\n",
    "        # threshold the probability to generate mask: mask=1 if prob > out_threshold, set mask to uint8\n",
    "        out_mask_np = (masks_probs>out_threshold).cpu().numpy().astype('uint8')\n",
    "\n",
    "    return out_mask_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(blocks, unfold_shape):\n",
    "\n",
    "    blocks_orig = blocks.view(unfold_shape)\n",
    "    output_c = unfold_shape[1] * unfold_shape[4]\n",
    "    output_h = unfold_shape[2] * unfold_shape[5]\n",
    "    output_w = unfold_shape[3] * unfold_shape[6]\n",
    "    blocks_orig = blocks_orig.permute(0, 1, 4, 2, 5, 3, 6).contiguous()\n",
    "    blocks_orig = blocks_orig.view(1, output_c, output_h, output_w)\n",
    "    # Remove the dimension at 0th position and convert to numpy\n",
    "    blocks_orig = blocks_orig.squeeze(0).detach().numpy()\n",
    "    return blocks_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image from testing dataset\n",
    "test_img_paths = \"data/TrainingDataset_MSSEG/08037ROGU/3DFLAIR.nii.gz\"\n",
    "test_mask_paths = \"data/TrainingDataset_MSSEG/08037ROGU/ManualSegmentation_1.nii.gz\"\n",
    "vol = nib.load(test_img_paths)\n",
    "vol_affine = vol.affine\n",
    "m = nib.load(test_mask_paths)\n",
    "img = np.array(vol.get_data(), np.float32) / 255.0\n",
    "img_padded = zero_padding(img, block_size)\n",
    "mask_orig = np.array(m.get_data(),np.uint8)\n",
    "#mask_padded = zero_padding(mask, block_size)\n",
    "\n",
    "# Generate data blocks of block_size\n",
    "img_blocks, unfold_shape_img = get_data_blocks(data = img_padded, block_size = block_size)\n",
    "#mask_blocks, unfold_shape_mask = get_data_blocks(data = mask_padded, block_size = block_size)\n",
    "\n",
    "img_array = img_blocks.numpy()\n",
    "test_img = []\n",
    "\n",
    "\n",
    "for i in range(len(img_array[0])):\n",
    "    test_img.append(img_array[0][i])\n",
    "#test_img_mask = preprocess_image(test_img_paths)\n",
    "\n",
    "orig_shape = len(test_img)\n",
    "print(len(test_img))\n",
    "\n",
    "#img_resize = cv2.resize(test_img_mask,(100,80))\n",
    "predicted = []\n",
    "for image in test_img:\n",
    "\n",
    "    # Predict the mask\n",
    "    mask_pred = predict_img(net=net,full_img=image, out_threshold=0.5)\n",
    "    # Rescale the mask back to original image size\n",
    "    #print(mask_pred.shape)\n",
    "    predicted.append(mask_pred)\n",
    "\n",
    "pred = torch.from_numpy(np.asarray(predicted)).type(torch.FloatTensor)\n",
    "mask_recon = reconstruct(pred,unfold_shape_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Segment\n",
    "mask_recon = torch.from_numpy(np.asarray(mask_recon)).type(torch.FloatTensor)\n",
    "mask_interpolated = nn.functional.interpolate(mask_recon.unsqueeze(0).unsqueeze(0), size=img.shape, mode='trilinear')\n",
    "img_seg = mask_interpolated * img\n",
    "img_seg = img_seg.squeeze().squeeze().numpy()\n",
    "img_seg_original = mask_orig*img\n",
    "\n",
    "print(img_seg.shape)\n",
    "print(img.shape)\n",
    "print(img_seg_original.shape)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def show_slices(slices):\n",
    "    \"\"\" Function to display row of image slices \"\"\"\n",
    "    fig, axes = plt.subplots(1, len(slices), figsize=(10,10))\n",
    "    for i, slice in enumerate(slices):\n",
    "        axes[i].imshow(slice.T, cmap=\"gray\", origin=\"lower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_0 = img[:, :, 87]\n",
    "slice_1 = img[:, :, 174]\n",
    "slice_2 = img[:, :, 261]\n",
    "show_slices([slice_0, slice_1, slice_2])\n",
    "plt.suptitle(\"Center slices for MRI image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_0 = img_seg[:, :, 87]\n",
    "slice_1 = img_seg[:, :, 174]\n",
    "slice_2 = img_seg[:, :, 200]\n",
    "show_slices([slice_0, slice_1, slice_2])\n",
    "plt.suptitle(\"Center slices for MRI image segmented by prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_0 = img_seg_original[:, :, 87]\n",
    "slice_1 = img_seg_original[:, :, 174]\n",
    "slice_2 = img_seg_original[:, :, 261]\n",
    "show_slices([slice_0, slice_1, slice_2])\n",
    "plt.suptitle(\"Center slices for MRI image segmented original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t = time.localtime()\n",
    "current_time = time.strftime(\"%H:%M:%S\", t)\n",
    "print(current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
