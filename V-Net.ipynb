{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELU(elu, nchan):\n",
    "    if elu:\n",
    "        return nn.ELU(inplace=True)\n",
    "    else:\n",
    "        return nn.PReLU(nchan)\n",
    "    \n",
    "def n_conv(nchan, depth, elu):\n",
    "    layers = []\n",
    "    for _ in range(depth):\n",
    "        layers.append(single_conv(nchan, elu))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class single_conv(nn.Module):\n",
    "    def __init__(self, nchan, elu):\n",
    "        super(single_conv, self).__init__()\n",
    "        self.relu = ELU(elu, nchan)\n",
    "        self.conv = nn.Conv3d(nchan, nchan, kernel_size=5, padding=2)\n",
    "        self.bn = nn.BatchNorm3d(nchan)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn(self.conv(x)))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class input_layer(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, elu):\n",
    "        super(input_layer, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv3d(in_ch,out_ch,kernel_size=5,padding=2)\n",
    "        self.relu = ELU(elu, out_ch)\n",
    "        self.bn = nn.BatchNorm3d(out_ch)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 1. Convolve\n",
    "        out = self.conv(x)\n",
    "        # 2. Normalize\n",
    "        out = self.bn(out)\n",
    "        # 3. Concat output accross 16 channels\n",
    "        x16 = torch.cat((x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x), 0)\n",
    "        # 4. Add concatenated output and input\n",
    "        out = torch.add(out, x16)\n",
    "        # 5. Activation\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class down_layer(nn.Module):\n",
    "    def __init__(self, in_ch, nConv, elu):\n",
    "        super(down_layer, self).__init__()\n",
    "        out_ch = 2*in_ch\n",
    "        self.down_conv = nn.Conv3d(in_ch, out_ch, kernel_size=2, stride=2)\n",
    "        self.bn = nn.BatchNorm3d(out_ch)\n",
    "        self.relu = ELU(elu, out_ch)\n",
    "        self.layers = n_conv(out_ch, nConv, elu)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        down = self.relu1(self.bn(self.down_conv(x)))\n",
    "        out = self.layers(out)\n",
    "        out = self.relu2(torch.add(out, down))\n",
    "        return down, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class up_layer(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, nConv, elu):\n",
    "        super(up_layer, self).__init__()\n",
    "        self.up_conv = nn.ConvTranspose3d(in_ch, out_ch // 2, kernel_size=2, stride=2)\n",
    "        self.bn = nn.BatchNorm3d(out_ch // 2)\n",
    "        self.relu1 = ELU(elu, out_ch // 2)\n",
    "        self.relu2 = ELU(elu, out_ch)\n",
    "        self.layers = n_conv(out_ch, nConv, elu)\n",
    "\n",
    "    def forward(self, x, skipx):\n",
    "        out = self.relu1(self.bn(self.up_conv(out)))\n",
    "        xcat = torch.cat((out, skipx), 1)\n",
    "        out = self.layers(xcat)\n",
    "        out = self.relu2(torch.add(out, xcat))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class output_layer(nn.Module):\n",
    "    def __init__(self, in_ch, elu, nll):\n",
    "        super(output_layer, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_ch, 2, kernel_size=5, padding=2)\n",
    "        self.bn = nn.BatchNorm3d(2)\n",
    "        self.conv2 = nn.Conv3d(2, 2, kernel_size=1)\n",
    "        self.relu1 = ELU(elu, 2)\n",
    "        if nll:\n",
    "            self.softmax = F.log_softmax\n",
    "        else:\n",
    "            self.softmax = F.softmax\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu1(self.bn(self.conv1(x)))\n",
    "        out = self.conv2(out)\n",
    "        out = out.permute(0, 2, 3, 4, 1).contiguous()\n",
    "        out = out.view(out.numel() // 2, 2)\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, elu=True, nll=False):\n",
    "        super(VNet, self).__init__()\n",
    "        #In\n",
    "        self.input = input_layer(1, 16, elu)\n",
    "        \n",
    "        #Down\n",
    "        self.down32 = down_layer(16, 2, elu)\n",
    "        self.down64 = down_layer(32, 3, elu)\n",
    "        self.down128 = down_layer(64, 3, elu)\n",
    "        self.down256 = down_layer(128, 3, elu)\n",
    "        \n",
    "        #Up\n",
    "        self.up256 = up_layer(256,256, 3, elu)\n",
    "        self.up128 = up_layer(128,128, 3, elu)\n",
    "        self.up64 = up_layer(64,64, 2, elu)\n",
    "        self.up32 = up_layer(32,32, 1, elu)\n",
    "        \n",
    "        #Out\n",
    "        self.output = output_layer(16, elu, nll)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #Layer 1: In\n",
    "        out16 = self.input(x)\n",
    "        \n",
    "        #Layer 2 : Down ( 2 conv layers deep)\n",
    "        d_32, out32 = self.down32(out16)\n",
    "        \n",
    "        #Layer 3 : Down ( 3 conv layers deep)\n",
    "        d_64, out64 = self.down64(out32)\n",
    "        \n",
    "        #Layer 4 : Down ( 3 conv layers deep)\n",
    "        d_128, out128 = self.down128(out64)      \n",
    "        \n",
    "        #Layer 5 : Down ( 3 conv layers deep)\n",
    "        d_256, out256 = self.down256(out128)\n",
    "        \n",
    "        #Layer 5 : up ( 3 conv layers deep)\n",
    "        output = self.up256(out256, out128)\n",
    "        \n",
    "        #Layer 4 : up ( 3 conv layers deep)\n",
    "        output = self.up128(output, out64)\n",
    "        \n",
    "        #Layer 3 : up ( 3 conv layers deep)\n",
    "        output = self.up64(output, out32)\n",
    "        \n",
    "        #Layer 2 : up ( 2 conv layers deep)\n",
    "        output = self.up32(output, out16)\n",
    "        \n",
    "        #Layer 1 : out\n",
    "        output = self.output(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = VNet(elu=True, nll=False)\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network:  65096700\n"
     ]
    }
   ],
   "source": [
    "n_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print('Number of parameters in network: ', n_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\priya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\dicom\\__init__.py:53: UserWarning: \n",
      "This code is using an older version of pydicom, which is no longer \n",
      "maintained as of Jan 2017.  You can access the new pydicom features and API \n",
      "by installing `pydicom` from PyPI.\n",
      "See 'Transitioning to pydicom 1.x' section at pydicom.readthedocs.org \n",
      "for more information.\n",
      "\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:  (144, 512, 512)\n",
      "Mask:  (144, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import os\n",
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Function, Variable\n",
    "from tqdm import tqdm\n",
    "\n",
    "# get all the image and mask path and number of images\n",
    "image_paths = glob.glob(\"train/*.gz\")\n",
    "mask_paths = glob.glob(\"train_masks/*.gz\")\n",
    "\n",
    "# split these path using a certain percentage\n",
    "for path in image_paths:\n",
    "    img = nib.load(path)\n",
    "    data = img.get_fdata()\n",
    "    print('Data: ',data.shape)\n",
    "    break\n",
    "    \n",
    "for path in mask_paths:\n",
    "    img = nib.load(path)\n",
    "    data = img.get_fdata()\n",
    "    print('Mask: ',data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val(image_paths, mask_paths, train_size):\n",
    "    img_paths_dic = {}\n",
    "    mask_paths_dic = {}\n",
    "    len_data = len(image_paths)\n",
    "    print('total len:', len_data)\n",
    "    for i in range(len(image_paths)):\n",
    "        img_paths_dic[os.path.basename(image_paths[i])[8:]] = image_paths[i]\n",
    "\n",
    "    for i in range(len(mask_paths)):\n",
    "        mask_paths_dic[os.path.basename(mask_paths[i])[19:]] = mask_paths[i]\n",
    "        \n",
    "    img_mask_list = []\n",
    "    #print(img_paths_dic)\n",
    "    #print(mask_paths_dic)\n",
    "    for key in img_paths_dic:\n",
    "        img_mask_list.append((img_paths_dic[key], mask_paths_dic[key]))\n",
    "        \n",
    "    train_img_mask_paths = img_mask_list[:int(len_data*train_size)] \n",
    "    val_img_mask_paths = img_mask_list[int(len_data*train_size):]\n",
    "    return train_img_mask_paths, val_img_mask_paths\n",
    "    #return img_mask_list\n",
    "    \n",
    "    \n",
    "def preprocess_image(image_mask_paths):\n",
    "    img_mask_list = []\n",
    "    #new_h, new_w = 80, 100\n",
    "    \n",
    "    for i in tqdm(range(len(image_mask_paths))):\n",
    "        vol = nib.load(image_mask_paths[i][0])\n",
    "        m = nib.load(image_mask_paths[i][1])\n",
    "        img = np.array(vol.dataobj, np.float32) / 255.0 \n",
    "        mask = np.array(m.dataobj,np.uint8)\n",
    "        \n",
    "        #print(img)\n",
    "        # Use cv2 to resize images to 80x100, use INTER_CUBIC interpolation\n",
    "        #img_resize = cv2.resize(img, dsize=(new_w, new_h), interpolation=cv2.INTER_CUBIC)\n",
    "        #mask_resize = np.uint8(cv2.resize(mask, dsize=(new_w, new_h), interpolation=cv2.INTER_CUBIC))\n",
    "        \n",
    "        img_mask_list.append((img, mask))\n",
    "    return img_mask_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total len: 7\n",
      "train len: 6\n"
     ]
    }
   ],
   "source": [
    "train_img_mask_paths, val_img_mask_paths = split_train_val(image_paths, mask_paths, 0.95)\n",
    "\n",
    "\n",
    "def pickle_store(file_name,save_data):\n",
    "    fileObj = open(file_name,'wb')\n",
    "    pickle.dump(save_data,fileObj)\n",
    "    fileObj.close()\n",
    "\n",
    "train_img_masks_save_path = './train_img_masks.pickle'\n",
    "if os.path.exists(train_img_masks_save_path):\n",
    "    with open(train_img_masks_save_path,'rb') as f:\n",
    "        train_img_masks = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    train_img_masks = preprocess_image(train_img_mask_paths)\n",
    "    pickle_store(train_img_masks_save_path,train_img_masks)\n",
    "print('train len: {}'.format(len(train_img_masks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val len: 1\n"
     ]
    }
   ],
   "source": [
    "# For validation data!!!!!\n",
    "\n",
    "\n",
    "val_img_masks_save_path = '.val_img_masks.pickle'\n",
    "if os.path.exists(val_img_masks_save_path):\n",
    "    with open(val_img_masks_save_path,'rb') as f:\n",
    "        val_img_masks = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    val_img_masks = preprocess_image(val_img_mask_paths)\n",
    "    pickle_store(val_img_masks_save_path,val_img_masks)\n",
    "print('val len: {}'.format(len(val_img_masks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"\n",
    "    Convert ndarrays in sample to Tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['img'], sample['label']\n",
    "        image = image[None,:,:]\n",
    "        label = label[None,:,:]\n",
    "        return {'img': torch.from_numpy(image.copy()).type(torch.FloatTensor),\n",
    "                'label': torch.from_numpy(label.copy()).type(torch.FloatTensor)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_masks, transforms=None): \n",
    "\n",
    "        self.image_masks = image_masks\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self):  # return count of sample we have\n",
    "\n",
    "        return len(self.image_masks)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        image = self.image_masks[index][0] # H, W, C\n",
    "        mask = self.image_masks[index][1]\n",
    "        \n",
    "#         image = np.transpose(image, axes=[2, 0, 1]) # C, H, W\n",
    "        \n",
    "        sample = {'img': image, 'label': mask}\n",
    "        \n",
    "        if transforms:\n",
    "            sample = self.transforms(sample)\n",
    "            \n",
    "        return sample\n",
    "\n",
    "train_dataset = CustomDataset(train_img_masks, transforms=transforms.Compose([ToTensor()]))\n",
    "val_dataset = CustomDataset(val_img_masks, transforms=transforms.Compose([ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define dice coefficient \n",
    "class DiceCoeff(Function):\n",
    "    \"\"\"Dice coeff for one pair of input image and target image\"\"\"\n",
    "    def forward(self, prediction, target):\n",
    "        self.save_for_backward(prediction, target)\n",
    "        eps = 0.0001 # in case union = 0\n",
    "        # Calculate intersection and union. \n",
    "        # You can convert the input image into a vector with input.contiguous().view(-1)\n",
    "        # Then use torch.dot(A, B) to calculate the intersection.\n",
    "        A = prediction.view(-1)\n",
    "        B = target.view(-1)\n",
    "        inter = torch.dot(A.float(),B.float())\n",
    "        union = torch.sum(A.float()) + torch.sum(B.float()) - inter + eps\n",
    "        # Calculate DICE \n",
    "        d = inter / union\n",
    "        return d\n",
    "\n",
    "# Calculate dice coefficients for batches\n",
    "def dice_coeff(prediction, target):\n",
    "    \"\"\"Dice coeff for batches\"\"\"\n",
    "    s = torch.FloatTensor(1).zero_()\n",
    "    \n",
    "    # For each pair of input and target, call DiceCoeff().forward(prediction, target) to calculate dice coefficient\n",
    "    # Then average\n",
    "    for i, (a,b) in enumerate(zip(prediction, target)):\n",
    "        s += DiceCoeff().forward(a,b)\n",
    "    s = s / (i + 1)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_net(net, dataset):\n",
    "    # set net mode to evaluation\n",
    "    net.eval()\n",
    "    tot = 0\n",
    "    for i, b in enumerate(dataset):\n",
    "        img = b['img'].to(device)\n",
    "        B = img.shape[0]\n",
    "        true_mask = b['label'].to(device)\n",
    "        # Feed the image to the network to get predicted mask\n",
    "        mask_pred = net(img.float())\n",
    "        \n",
    "        # For all pixels in predicted mask, set them to 1 if larger than 0.5. Otherwise set them to 0\n",
    "        mask_pred = mask_pred > 0.5\n",
    "        # calculate dice_coeff()\n",
    "        # note that you should add all the dice_coeff in validation/testing dataset together \n",
    "        # call dice_coeff() here\n",
    "        tot += dice_coeff(true_mask,mask_pred)\n",
    "        # Return average dice_coeff()\n",
    "    return tot / (i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/5.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 18874368000 bytes. Buy new RAM!\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-d06a684df1b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# Feed your images into the network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mmasks_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[1;31m#masks_pred = nn.functional.interpolate(masks_pred, size=(80,100), mode='bilinear')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m# Flatten the predicted masks and true masks. For example, A_flat = A.view(-1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-a842b15f96d7>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m#Layer 1: In\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mout16\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m#Layer 2 : Down ( 2 conv layers deep)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\priya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-906fa44c6e75>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# 1. Convolve\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;31m# 2. Normalize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\priya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\priya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    478\u001b[0m                             self.dilation, self.groups)\n\u001b[0;32m    479\u001b[0m         return F.conv3d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 480\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 18874368000 bytes. Buy new RAM!\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "epochs = 5 # e.g. 10, or more until dice converge\n",
    "batch_size = 1 # e.g. 16\n",
    "lr = 0.01        # e.g. 0.01\n",
    "N_train = len(train_img_masks)\n",
    "model_save_path = './model/'  # directory to same the model after each epoch. \n",
    "\n",
    "optimizer = optim.SGD(net.parameters(),lr = lr,momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Start training\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch {}/{}.'.format(epoch + 1, epochs))\n",
    "    net.train()\n",
    "    # Reload images and masks for training and validation and perform random shuffling at the begining of each epoch\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    for i, b in enumerate(train_loader):\n",
    "        # Get images and masks from each batch\n",
    "        \n",
    "        imgs = b['img'].to(device)\n",
    "        true_masks = b['label'].to(device)\n",
    "        \n",
    "        # Feed your images into the network\n",
    "        \n",
    "        masks_pred = net.forward(imgs.float())\n",
    "        #masks_pred = nn.functional.interpolate(masks_pred, size=(80,100), mode='bilinear')\n",
    "        # Flatten the predicted masks and true masks. For example, A_flat = A.view(-1)\n",
    "        masks_probs_flat = masks_pred.view(-1)\n",
    "        true_masks_flat = true_masks.view(-1)\n",
    "        # Calculate the loss by comparing the predicted masks vector and true masks vector\n",
    "        # And sum the losses together \n",
    "        loss = criterion(masks_probs_flat,true_masks_flat.float())\n",
    "        epoch_loss += loss.item()\n",
    "        if count % 50 == 0:\n",
    "            print('{0:.4f} --- loss: {1:.6f}'.format(i * batch_size / N_train, loss.item()))\n",
    "        count = count + 1\n",
    "        # optimizer.zero_grad() clears x.grad for every parameter x in the optimizer. \n",
    "        # It’s important to call this before loss.backward(), otherwise you’ll accumulate the gradients from multiple passes.\n",
    "        optimizer.zero_grad()\n",
    "        # loss.backward() computes dloss/dx for every parameter x which has requires_grad=True. \n",
    "        # These are accumulated into x.grad for every parameter x\n",
    "        loss.backward()\n",
    "        # optimizer.step updates the value of x using the gradient x.grad.\n",
    "        optimizer.step()\n",
    "    print('Epoch finished ! Loss: {}'.format(epoch_loss / i))\n",
    "    \n",
    "    # Perform validation with eval_net() on the validation data\n",
    "    val_dice = eval_net(net,val_loader)\n",
    "    print('Validation Dice Coeff: {}'.format(val_dice))\n",
    "    # Save the model after each epoch\n",
    "    if os.path.isdir(model_save_path):\n",
    "        torch.save(net.state_dict(),model_save_path + 'Car_Seg_Epoch{}.pth'.format(epoch + 1))\n",
    "    else:\n",
    "        os.makedirs(model_save_path, exist_ok=True)\n",
    "        torch.save(net.state_dict(),model_save_path + 'Car_Seg_Epoch{}.pth'.format(epoch + 1))\n",
    "    print('Checkpoint {} saved !'.format(epoch + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
